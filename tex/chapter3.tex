\chapter{Related Work}\label{cha:related_work}

In this chapter four major pieces of major work is going to be described.
Section~\ref{sec:lvars} describes the LVars system and LVish, the extension
of LVars which introduces the concepts of quiescence and freezing. 
Section~\ref{sec:reactive_async} describes Reactive Async, a programming model
inspired by the extended LVars system. In section~\ref{sec:lacasa} the LaCasa
system is introduced, and finally the concept of spores is briefly introduced in
section~\ref{sec:spores}.

\section{LVars}\label{sec:lvars}

LVars~\parencite{kuper2013lvars} is a programming model that was introduced as a
solution to the problem of guaranteed deterministic concurrent programs. It
generalizes the concept of write-once data
structures~\parencite{nikhil1989structures}, also called IVars, with the ability
to write more than once but limiting update operations to being monotonic. I.e.
the values taken by LVars are part of a programmer specified lattice and all
updates are done through a join operation of the old and new values. This
ensures that writes commute~\parencite{kuper2013lvars}.

\subsection{Stores \& Lattice}%
\label{sub:stores_and_lattice}

At the foundation of the LVars system lays lattices. The values resulting from a
computation is going to be elements from a lattice $\LVarsLat$, specified by the
programmer. These lattice values are stored in a \emph{store}.  This is a set of
pairs consisting of a location and a lattice element. For a location there is
at most one value. Letting $\LVarsLoc$ be the set of locations, a store $S$ can
be represented using a partial map
\begin{equation*}
  S: \LVarsLoc \rightharpoonup \LVarsLat.
\end{equation*}

\subsection{LVars Operations}%
\label{sub:lvars_operations}

The LVars model supports three main operations~\parencite{kuper2013lvars}.
\begin{itemize}
  \item Extending the store with a new location. This takes a fresh location and
    sets its value to $\bot_{\LVarsLat}$, the bottom element of $\LVarsLat$.
  \item Updating a store location, also called a \emph{put} operation. This
    operation takes a store location $r$ and a lattice value $l$. Given a store
    $S$ this updates location $r$ with $l \sqcup S(r)$. To ensure determinism,
    any put that takes a store location to $\top_{\LVarsLat}$ results in an
    error.
  \item A read operation also referred to as \emph{get}. This operation is
    further specified with a threshold set, i.e. a set of lattice values, and a
    store location. The operation blocks until the store location has passed one
    of the values in the threshold set, upon which it returns this value. In
    order to ensure determinism the elements in the threshold set are required
    to be mutually \emph{incompatible}. Two elements $l_1, l_2$ are incompatible if
    \begin{equation*}
      l_1 \sqcup l_2 = \top_{\LVarsLat}
    \end{equation*}
    where $\top$ is the top element of $\LVarsLat$.
\end{itemize}

\subsection{LVish: Extending LVars}%
\label{sub:lvish_extending_lvars}

LVish~\parencite{kuper2014freeze} is an extension of LVars. It introduces a new
operation called freezing, a system to create dependencies between store
locations, and the concepts of quasi determinism and quiescence.  In LVish the
store is modified such that every location also has an associated \emph{freeze
bit} with a default value of $\LVarsFalse$. The \emph{freeze} operation
takes a location and changes its associated bit to $\LVarsTrue$.  Whenever a
freeze bit is $\LVarsTrue$ it prevents the associated lattice value from being
changed. If a put operation tries to modify a freezed store location the
computation ends up in an error state, $\Error$.

As mentioned LVish also permits the programmer to create dependencies between
store locations. To do this the programmer specifies a location together with a
set of threshold lattice values and a callback function. The callback is
executed as soon as the given location passes a threshold value. Upon execution
the callback is given access to the threshold passed. Unlike the threshold sets
for the get operation the elements does not have to be incompatible. 

With the introduction of freezing a program could take different execution
paths where one halts in $\Error$ and the other halts in a non-erroneos state.
It is thus obvious that the LVish system is no longer deterministic. Therefore
the authors introduce \emph{quasi determinism}, a weaker form of
determinism. Instead of requiring the same computation result for all execution
paths, quasi determinism only requires that results be the same for
non-erroneous halting states.

Finally the LVish system makes use of a concept called \emph{quiescence}, in
order to decide when to freeze a location. In short a computation reaches
quiescence when no more changes to the store are expected to take place. For
example, this could be when there are no more callback threads running.

The proof of quasi determinism for LVish presented by
\textcite{kuper2014freezeTR} is flawed. This will be touched upon further in
chapter~\ref{cha:challenges}.


\section{Reactive Async}\label{sec:reactive_async}

% TODO add ref to scala language
% TODO add ref to RA article or thesis.
Reactive Async is a Scala programming framework inspired by LVish. It is based
around objects called \emph{cells}.  Cells are similar to the store locations
from LVish: They hold a value from a lattice, the value is updated through a
join operation and you are able to create dependencies between them using
callbacks.

Like in LVish the cells are updated through a put operation.  The cells and
dependencies can be illustrated as a graph.  Figure~\ref{fig:ra_example} is a
very simple example of a dependency graph.  There are two cells $c_1$ and $c_2$
and a dependency callback $f$ between them. This should be interpreted as $c_2$
being dependent on $c_1$.  $f$ is a function which takes a lattice value and
returns either nothing or a new lattice value.  As soon as the lattice value of
$c_1$ is updated to a new lattice value $l$, the callback $f(l)$ runs. If $f(l)$
returns a lattice value $l'$, $c_2$ is updated with $l'$ though a put operation.
If the result of $f(l)$ is nothing, nothing is done with the lattice value of
$c_2$.

If a programmer is not careful, there are several aspects of Reactive Async
which can make a computation non-deterministic. This will be further discussed
in chapter~\ref{cha:challenges}. This work aims to construct a formal model with
proven determinism properties which could be used as a basis for future
revisions of Reactive Async.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node[circle,draw] (c1) at (0, 0) {$c_1$};
    \node[circle,draw] (c2) at (3, 0) {$c_2$};
    \draw[-{Latex[length=0.3cm]}] (c1)  -- node[above] {$f$}  (c2);
  \end{tikzpicture} 
  \caption{Simple Reactive Async dependency graph.}
  \label{fig:ra_example}
\end{figure}

\section{LaCasa}\label{sec:lacasa}

LaCasa\parencite{conf/oopsla/HallerL16} is a programming model for ensuring
determinism in an actor model\parencite{actormodelref} setting. It draws
parallels to the OCAP model to ensure isolation between threads.  In this
section we give a brief introduction to the type system of LaCasa and the
intuition behind it.

The formalization of LaCasa is based on a simple object oriented language.
\textcite{conf/oopsla/HallerL16} presents several models, the last of which
models a concurrent setting with several parallel processes. In order to ensure
determinism all data interchanged between processes must be encapsulated in
\emph{boxes}. A box is a container for a class reference with some accompanying
methods to access and perform calculations with the encapsulated object. The
type system ensures that all calculations made with objects encapsulated in a box
are isolated, i.e., that at most one thread can access the data and perform
calculations with it.

A big part of ensuring isolation is requiring all objects encapsulated in boxes
are of classes typed as \emph{ocap}. This basically means three things.
\begin{enumerate}
  \item The methods never access a global global object.
  \item All field types are ocap.
  \item Method can only create new instances of ocap classes.
\end{enumerate}
To ensure this holds in the presence of a subtyping relation, superclasses are
furthermore required to be ocap~\parencite{conf/oopsla/HallerL16}.

These rules ensure that all references accuired by an ocap object will have been
explicitly given to it. The ocap classification for classes will later be
incorporated in the system presented in this thesis.

% Describe the basic ideas of lacasa and the idea of using OCAP constraints to 
% assure determinism

\section{Spores}\label{sec:spores}

% TODO add some references
In recent years there has been an increasing demand for distributed and
concurrent data processing. Highly concurrent or distributed frameworks like
Akka and Spark have become more and more popular. As their popularity grow
concerns have been raised over hazards that relate to closures, one of the basic
and most important constructs in modern programming languages. A closure is
basically the notion of capturing and using an external variable in a function
definition. In a concurrent or distributed setting this could be fatal if, e.g.,
we capture mutable or non-serializable references. 

An experimental solution for Scala is
\emph{spores}~\parencite{conf/ecoop/MillerHO14}. Basically, spores allow the
programmer to clearly declare what is being captured and furthermore limit what
types that can be captured. The use of spores can be forced in code by using the
spore type instead of the normal function type when declaring methods and
similar.

% Describe the basic ideas of spores: (dis)allow certain types of captures to
% enforce certain properties



